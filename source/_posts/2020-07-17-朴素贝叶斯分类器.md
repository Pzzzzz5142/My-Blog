---
title: 朴素贝叶斯分类器
date: 2020-07-17 00:28:13
mathjax: true
tags:
- 学习
- 机器学习
- 基础
categories:
- 学习
- 知识图谱
---

# 朴素贝叶斯分类器

> 分类，就是分类。或是分为 1，抑或是分为 0。

<!--more-->

## 原理

计算机的分类大致上便是给定特征，然后输出结果。

而贝叶斯分类器本身就是一种基于条件概率公式的分类器。

$$P(A|C)=\frac{P(C|A)*P(A)}{P(C)}$$

大致转换为人话就是：

$$P(类别|特征)=\frac{P(特征|类别)*P(类别)}{P(特征)}$$

这个公式有什么好处呢？那就是$P(特征|类别)$、$P(类别 )$，和$P(特征)$都是可以通过数据集统计到的，当数据集够大，那么我们也就能够通过数据集本身统计到的频率来模拟概率。从而当我们给定一个特征的时候，我们能够根据统计到的信息计算在有这个特征的条件下得到的概率是多少。得到了这个条件概率，我们自然能根据这个概率对这个特征进行分类。

当然，我们也不可能只有一个特征。而这就是朴素贝叶斯拉垮的其中一个点了，他假设特征是不相关的。即认为：

$$P(特征)=P(特征_1)*P(特征_2)*...*P(特征_n)$$

那么上述公式便改写为：

$$P(类别|特征)=\frac{P(特征|类别)*P(类别)}{P(特征)}=\frac{P(类别)}{P(特征)}\prod_{i=0}^{n}P(特征_i)$$

这是一个有一定道理的假设。因为的确虽然特征之间难免有关联，但是一般来说关联没有那么的大。而且如果计算机要去真正的处理这些关联的话，那计算量可是相当的恐怖。

## Code

计算机届一个很著名的话便是：

> What i can‘t code is what i don't know

那么我们话不多说直接开始代码部分。这次的代码部分我采用的UCI的adult数据集。数据地址在[这里](http://archive.ics.uci.edu/ml/datasets/Adult)。

### 数据集的含义

该数据从美国1994年人口普查数据库中抽取而来，因此也称作“人口普查收入”数据集，共包含48842条记录，年收入大于50k的占比23.93\%，年收入小于50k的占比76.07\%，数据集已经划分为训练数据32561条和测试数据16281条。该数据集类变量为年收入是否超过50k，属性变量包括年龄、工种、学历、职业等14类重要信息，其中有8类属于类别离散型变量，另外6类属于数值连续型变量。该数据集是一个分类数据集，用来预测年收入是否超过50k。 

| 数据集特征： | 多变量         | 记录数：   | 48842 | 领域：       | 社会       |
| ------------ | -------------- | ---------- | ----- | ------------ | ---------- |
| 属性特征：   | 类别型、整数型 | 属性数目： | 14    | 捐赠日期：   | 1996-05-01 |
| 相关应用：   | 分类           | 缺失值？   | 有    | 网站点击数： | 1059012    |

14个属性变量具体介绍如下：

| 属性名         | 类型       | 含义         |
| -------------- | ---------- | ------------ |
| age            | continuous | 年龄         |
| workclass      | discrete   | 工作类别     |
| fnlwgt         | continuous | 序号         |
| education      | discrete   | 受教育程度   |
| education-num  | continuous | 受教育时间   |
| marital-status | discrete   | 婚姻状况     |
| occupation     | discrete   | 职业         |
| relationship   | discrete   | 社会角色     |
| race           | discrete   | 种族         |
| sex            | discrete   | 性别         |
| capital-gain   | continuous | 资本收益     |
| capital-loss   | continuous | 资本支出     |
| hours-per-week | continuous | 每周工作时间 |
| native-country | discrete   | 国籍         |

（这一小节全部抄自[这里](https://blog.csdn.net/hohaizx/java/article/details/79084774)

### 数据的处理！

一般而言数据预处理是比较简单的项目，但是这次我我是直接在数据读取的时候就统计特征频率，因此还是有单独列出来讲解的必要。

大致思路就是用一个三维的矩阵来存储每一个特征的值对应的类别。如果是连续的值便对他进行离散化。

其中，dataMatrix第一维便是特征的；类别，第二维是类别中不同的值，第三维是一个固定的两个元素的列表，第一个元素代表年薪小于50k的值的个数，第二个元素代表年薪大于50k的个数。

#### 取了哪些特征？

当前取了如下特征：

1. 所有的原生离散型数据特征
2. 由于年龄最大为 90 岁，故以 10 岁为单位，共计9类进行离散化。
3. 由于工作时长最大为 99 小时，故以 5 小时为单位，共计20类进行离散化。
4. 遇到未知类别，则完全抛弃不处理。

#### 数据预处理的代码

```python
t = 0  # 年薪大于50k的人数
f = 0  # 年薪小于50k的人数
# 从文件中加载数据
def loadDataSet(path):
    global t, f
    dataMatrix = [[[0, 0] for _ in range(len(v) + 1)] for _, v in value.items()]
    # 这里给出了python 中读取文件的简便方式
    fl = open(path)
    for line in fl.readlines():
        # print(line)
        lineList = line.strip().split(",")
        if len(lineList) < 13:  # 处理空行和不和条件的行
            continue
        lineList = [i.strip() for i in lineList]
        if lineList[-1] == "<=50K":  # 统计 t 和 f
            t += 1
        else:
            f += 1
        for i in range(len(label)):
            try:  # 因为有离散的值，所以用 try 语句块来分别处理离散的值和连续的值
                dataMatrix[i][value[label[i]].index(lineList[i])][
                    1 if lineList[-1].strip() == "<=50K" else 0
                ] += 1  # value是一个类别和种类的字典。因为实在太长就没有列出。
                dataMatrix[i][-1][1 if lineList[-1].strip() == "<=50K" else 0] += 1
            except:
                if label[i] == "hours-per-week":
                    if len(dataMatrix[i]) == 2:
                        dataMatrix[i] = [[0, 0] for _ in range(21)]
                    dataMatrix[i][int(float(lineList[i]) / 5)][  # 以5为单位，离散化连续数值
                        1 if lineList[-1].strip() == "<=50K" else 0
                    ] += 1
                    dataMatrix[i][-1][1 if lineList[-1].strip() == "<=50K" else 0] += 1
                if label[i] == "age":
                    if len(dataMatrix[i]) == 2:
                        dataMatrix[i] = [[0, 0] for _ in range(11)]
                    dataMatrix[i][int(float(lineList[i]) / 10)][  # 以10为单位，离散化连续数值
                        1 if lineList[-1].strip() == "<=50K" else 0
                    ] += 1
                    dataMatrix[i][-1][1 if lineList[-1].strip() == "<=50K" else 0] += 1
    fl.close()
    return dataMatrix
```

#### 平滑处理

平滑处理就是非常暴力的通过对概率为0对值加一，然后总数加一

#### 平滑处理的代码

```	python
def smooth(Matrix):
    global t, f
    for i in Matrix:
        for ind, val in enumerate(i):
            if val[0] == 0:
                i[ind][0] += 1
                i[-1][0] += 1
                f += 1
            if val[1] == 0:
                i[ind][1] += 1
                i[-1][1] += 1
                t += 1
```

## 结果

搞完了代码，总得上个结果。

一开始叫我弄离散化连续值我是拒绝的，因为我实在懒得搞。

然后得到了这个结果：

![image-20200717010456670](image-20200717010456670.png)

实际上，由于这个数据集本身很大，所以出现0概率的情况属实太少了，因此带不带平滑处理貌似区别也不少很大。

但是我对这个结果不是很满意。于是我便加上了两个我认为重要的数据的离散化处理，并得到了如下结果：

![image-20200717010658444](image-20200717010658444.png)

这次准确率提升了3%，而f1值提升了将近10%！这可太好了！而同时，平滑处理本身也对结果有一定的优化，只是这个优化确实是有限的。

## 小小的问题

在讲原理的时候我便提到过，当特征之间不是无关的时候，那个假设可能是错误的。而这也的确在本次运行中出现了。当我们在测试函数中加入如下两行代码：

```python
assert Ok > 1
assert Not > 1
```

其中`Ok`是在该特征下，这个人年薪5w🔪以上的概率，`Not`变量含义则与之相反。那么我们继续运行一下。

![image-20200717011622886](image-20200717011622886.png)

啊哈，他报错了。这说明确实在这些变量之间，确实是有相关性的。但这也是朴素贝叶斯分类器无能为力的地方。举个例子，有如下关系：

| 熬夜吗？ | 情况       |
| -------- | ---------- |
| 熬夜     | 买新游戏了 |
| 不熬夜   | 没买新游戏 |

显然，只有一个变量的情况下，我们计算得到$P(买新游戏|熬夜)=\frac{P(熬夜|买新游戏)*P(买新游戏)}{P(熬夜)}=\frac{1*\frac{1}{2}}{\frac{1}{2}}$。

但是如果我们加一维数据：

| 熬夜吗？ | 出去玩   | 情况       |
| -------- | -------- | ---------- |
| 熬夜     | 出去     | 买新游戏   |
| 熬夜     | 出去     | 买新游戏   |
| 不熬夜   | 不出去玩 | 没买新游戏 |
| 熬夜     | 不出去玩 | 买新游戏   |

那么我们计算出$P(买新游戏|熬夜，出去玩)=\frac{P(熬夜，出去玩|买新游戏)*P(买新游戏)}{P(熬夜，出去玩)}=\frac{1*\frac{2}{3}}{\frac{3}{4}*\frac{1}{2}}=\frac{16}{9}$。

等等，这概率为什么会大于1？为什么我们加了一个毫不相干的数据他就炸了呢？但是仔细看这个数据就不难发现，熬夜的情况下有$\frac23$的情况是要出去玩的。也就是说虽然在实际生活中看起来这两维毫不相干，但是数学上我们看起来的确是相关的。这也是贝叶斯分类器的一个小局限所在。

## 总结

贝叶斯的优点很多：

1. 它足够简单，遇事不决先试试成本很低，而且效果也颇为不错。
2. 运行速度快
3. 数学原理明确，可解释性强，结果稳定

但是他的缺点也很明显：

1. **各特征之间无关，这个假设太强了点**
2. 模型有点简单，面对复杂的问题有些力不从心

